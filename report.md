# 实验报告-HW5
## Task 1
**Q1. The result of mapping is shuffled and sent to reducers, which could cost a lot of network traffic. Propose an approach to address this issue.**

One approach to address the high network traffic during the shuffle phase is using a combiner function. A combiner is a mini-reduce function that operates on the intermediate key-value pairs generated by the map tasks before they are sent over the network. The combiner function aggregates the data locally on the map worker, reducing the volume of data that needs to be transferred over the network.

In the MapReduce model, the combiner function can be the same as the reduce function if the operation is associative and commutative (e.g., summing word counts). By performing partial aggregation on the mapper’s local machine, the amount of intermediate data is significantly reduced, minimizing the overall network bandwidth consumption during the shuffle phase.

For example, in a word count application, instead of sending 〈word, 1〉 for every occurrence of a word, the combiner can sum these locally and only send one 〈word, count〉 per word to the reducer, greatly reducing the volume of data shuffled across the network.

**Q2. If a mapper or reducer task is too slow, which would make the whole map-reduce task very slow. Propose an approach to address this issue.**

One common approach to handle slow tasks, also known as "stragglers," is to use speculative execution. In speculative execution, when a task (either map or reduce) is taking much longer than expected to complete, the master node schedules a duplicate of that task on another worker node. The first instance to finish (either the original or the speculative task) is used, and the other one is killed.

This technique can help mitigate the effects of stragglers that slow down the entire MapReduce job due to hardware issues, network congestion, or resource contention on the worker machines. By duplicating slow tasks, speculative execution can ensure that the overall job completes faster even if some tasks are slow. This is particularly helpful when only a few tasks are slow, while the majority are completing quickly.

This approach was implemented successfully in Google's MapReduce system, as described in the paper.

## Task 2
### 实现方法
与WordCount类似，只是对map函数进行了简要的修改，将第二段字符串（即出发端点）作为key、1作为值进行计数。reduce函数没有改变，bash脚本也没有任何修改。

### 实验结果
见od_res文件夹下的文件。

## Task 3
### 实现方法
由于case1和case2需要计算得到的最大出度顶点数量不同，在main函数中添加对args[2]的识别及转换，作为待求顶点数量k传入reduce函数。
主体部分与Task2类似，map直接挪用Task2的map函数。reduce函数中，对于每个key，将其value进行累加，然后将key-value对存入一个priority queue中，队列的大小为k。当队列大小超过k时，将队列中的最小元素弹出，最终队列中的元素即为出度最大的k个顶点。为了保证顺序，将队列中的元素逐个弹出，然后存入一个vector中，并逆序输出。
bash脚本为“run_tk.sh”，在原有脚本基础上执行时需要额外传入k值。

### 实验结果
见tk_res文件夹下的文件。

## 一些看法
感觉本次实验难度并不大，主要时间耗费在熟悉java语言和hadoop框架上。